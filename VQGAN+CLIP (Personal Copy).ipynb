{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VQGAN+CLIP (Personal Copy).ipynb","private_outputs":true,"provenance":[{"file_id":"1xIVqHFbxIYoCipqxmYHT5OXClrSVS7gS","timestamp":1626199386101},{"file_id":"1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT","timestamp":1625957690437},{"file_id":"1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN","timestamp":1621115030940},{"file_id":"15UwYDsnNeldJFHJ9NdgYBYeo6xPmSelP","timestamp":1618333741979}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CppIQlPhhwhs"},"source":["# Generate images from text phrases with VQGAN and CLIP (z + quantize method with augmentations).\n","\n","Current version slightly modified from [this](https://colab.research.google.com/drive/1ud6KJeKdq5egQx_zz2-rni5R-Q-vxJdj?usp=sharing) notebook by [@angremlin](https://twitter.com/angremlin), which was based on the original VQGAN+CLIP [notebook](https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT?fbclid=IwAR30ZqxIJG0-2wDukRydFA3jU5OpLHrlC_Sg1iRXqmoTkEhaJtHdRi6H7AI#scrollTo=FhhdWrSxQhwg) by Katherine Crowson ([Github](https://github.com/crowsonkb), [Twitter](https://twitter.com/RiversHaveWings)).\n","\n","If you need help or would like to learn more, see Crowson's [help file](https://docs.google.com/document/d/1Lu7XPRKlNhBQjcKr8k8qRzUzbBW7kzxb5Vu72GMRn2E/edit#), this [Spanish help page](https://tuscriaturas.miraheze.org/wiki/Ayuda:Crear_im√°genes_con_VQGAN+CLIP), and/or Unicole's [Data Prophecy how-to video](https://youtu.be/1YXo1HAIOFo).\n","\n","I just added troubleshooting cells, more models to use (some hosted on my Google Drive), and some options for parameters and video generation.\n","\n","--Downy Thornapple"]},{"cell_type":"code","metadata":{"id":"VA1PHoJrRiK9","cellView":"form"},"source":["# @title Licensed under the MIT License\n","\n","# Copyright (c) 2021 Katherine Crowson\n","\n","# Permission is hereby granted, free of charge, to any person obtaining a copy\n","# of this software and associated documentation files (the \"Software\"), to deal\n","# in the Software without restriction, including without limitation the rights\n","# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","# copies of the Software, and to permit persons to whom the Software is\n","# furnished to do so, subject to the following conditions:\n","\n","# The above copyright notice and this permission notice shall be included in\n","# all copies or substantial portions of the Software.\n","\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n","# THE SOFTWARE."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkUfzT60ZZ9q","cellView":"form"},"source":["# @title Optional GPU check\n","# See help file here for explanation: https://medium.com/analytics-vidhya/explained-output-of-nvidia-smi-utility-fc4fbee3b124\n","\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAQUIvOBb7pX","cellView":"form"},"source":["# @title Optional RAM check\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSfISAhyPmyp","cellView":"form"},"source":["# @title Installing the libraries\n","# @markdown This cell will take a while because you have to download multiple libraries\n"," \n","print(\"Downloading CLIP...\")\n","!git clone https://github.com/openai/CLIP                 &> /dev/null\n"," \n","print(\"Installing Python libraries for AI...\")\n","!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n","!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n","!pip install kornia                                       &> /dev/null\n","!pip install einops                                       &> /dev/null\n","!pip install transformers                                 &> /dev/null\n"," \n","print(\"Installing libraries for metadata management ...\")\n","!pip install stegano                                      &> /dev/null\n","!apt install exempi                                       &> /dev/null\n","!pip install python-xmp-toolkit                           &> /dev/null\n","!pip install imgtag                                       &> /dev/null\n","!pip install pillow==7.1.2                                &> /dev/null\n"," \n","print(\"Installing python libraries for creating video ...\")\n","!pip install imageio-ffmpeg &> /dev/null\n","!mkdir steps\n","print(\"Installation finished.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EXMSuW2EQWsd","cellView":"form"},"source":["# @title Loading libraries and definitions\n","\n","import argparse\n","import math\n","from pathlib import Path\n","import sys\n","import hashlib\n","from IPython.display import Audio, clear_output\n","\n","sys.path.append('./taming-transformers')\n","from IPython import display\n","from base64 import b64encode\n","from omegaconf import OmegaConf\n","from PIL import Image\n","from taming.models import cond_transformer, vqgan\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from tqdm.notebook import tqdm\n","\n","from CLIP import clip\n","import kornia.augmentation as K\n","import numpy as np\n","import imageio\n","from PIL import ImageFile, Image\n","from imgtag import ImgTag    # metadatos \n","from libxmp import *         # metadatos\n","import libxmp                # metadatos\n","from stegano import lsb\n","import json\n","import re\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n"," \n","def sinc(x):\n","    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n","\n","def lanczos(x, a):\n","    cond = torch.logical_and(-a < x, x < a)\n","    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n","    return out / out.sum()\n","\n","def ramp(ratio, width):\n","    n = math.ceil(width / ratio + 1)\n","    out = torch.empty([n])\n","    cur = 0\n","    for i in range(out.shape[0]):\n","        out[i] = cur\n","        cur += ratio\n","    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n","\n","def resample(input, size, align_corners=True):\n","    n, c, h, w = input.shape\n","    dh, dw = size\n","\n","    input = input.view([n * c, 1, h, w])\n","\n","    if dh < h:\n","        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n","        pad_h = (kernel_h.shape[0] - 1) // 2\n","        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n","        input = F.conv2d(input, kernel_h[None, None, :, None])\n","\n","    if dw < w:\n","        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n","        pad_w = (kernel_w.shape[0] - 1) // 2\n","        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n","        input = F.conv2d(input, kernel_w[None, None, None, :])\n","\n","    input = input.view([n, c, h, w])\n","    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n","\n","class ReplaceGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x_forward, x_backward):\n","        ctx.shape = x_backward.shape\n","        return x_forward\n"," \n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        return None, grad_in.sum_to_size(ctx.shape)\n"," \n"," \n","replace_grad = ReplaceGrad.apply\n"," \n"," \n","class ClampWithGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, min, max):\n","        ctx.min = min\n","        ctx.max = max\n","        ctx.save_for_backward(input)\n","        return input.clamp(min, max)\n"," \n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        input, = ctx.saved_tensors\n","        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n"," \n"," \n","clamp_with_grad = ClampWithGrad.apply\n"," \n"," \n","def vector_quantize(x, codebook):\n","    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n","    indices = d.argmin(-1)\n","    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n","    return replace_grad(x_q, x)\n"," \n"," \n","class Prompt(nn.Module):\n","    def __init__(self, embed, weight=1., stop=float('-inf')):\n","        super().__init__()\n","        self.register_buffer('embed', embed)\n","        self.register_buffer('weight', torch.as_tensor(weight))\n","        self.register_buffer('stop', torch.as_tensor(stop))\n"," \n","    def forward(self, input):\n","        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n","        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n","        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n","        dists = dists * self.weight.sign()\n","        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n"," \n"," \n","def parse_prompt(prompt):\n","    vals = prompt.rsplit(':', 2)\n","    vals = vals + ['', '1', '-inf'][len(vals):]\n","    return vals[0], float(vals[1]), float(vals[2])\n"," \n"," \n","class MakeCutouts(nn.Module):\n","    def __init__(self, cut_size, cutn, cut_pow=1.):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.cut_pow = cut_pow\n","        self.augs = nn.Sequential(\n","            K.RandomHorizontalFlip(p=0.5),\n","            # K.RandomSolarize(0.01, 0.01, p=0.7),\n","            K.RandomSharpness(0.3,p=0.4),\n","            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n","            K.RandomPerspective(0.2,p=0.4),\n","            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n","        self.noise_fac = 0.1\n"," \n"," \n","    def forward(self, input):\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","        min_size = min(sideX, sideY, self.cut_size)\n","        cutouts = []\n","        for _ in range(self.cutn):\n","            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n","            offsetx = torch.randint(0, sideX - size + 1, ())\n","            offsety = torch.randint(0, sideY - size + 1, ())\n","            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n","        batch = self.augs(torch.cat(cutouts, dim=0))\n","        if self.noise_fac:\n","            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n","            batch = batch + facs * torch.randn_like(batch)\n","        return batch\n"," \n"," \n","def load_vqgan_model(config_path, checkpoint_path):\n","    config = OmegaConf.load(config_path)\n","    if config.model.target == 'taming.models.vqgan.VQModel':\n","        model = vqgan.VQModel(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","\n","    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n","        model = vqgan.GumbelVQ(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","\n","    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n","        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n","        parent_model.eval().requires_grad_(False)\n","        parent_model.init_from_ckpt(checkpoint_path)\n","        model = parent_model.first_stage_model\n","    else:\n","        raise ValueError(f'unknown model type: {config.model.target}')\n","    del model.loss\n","    return model\n","\n","def resize_image(image, out_size):\n","    ratio = image.size[0] / image.size[1]\n","    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n","    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n","    return image.resize(size, Image.LANCZOS)\n","\n","def download_img(img_url):\n","    try:\n","        return wget.download(img_url,out=\"input.jpg\")\n","    except:\n","        return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FhhdWrSxQhwg","cellView":"form"},"source":["#@title Selection of models to download\n","#@markdown By default, the notebook downloads Model 16384 from ImageNet. There are others such as ImageNet 1024, COCO-Stuff, WikiArt 1024, WikiArt 16384, FacesHQ or S-FLCKR, which are not downloaded by default, since it would be in vain if you are not going to use them, so if you want to use them, simply select the models to download.\n","\n","#This step is required even if you don't download a model. Just uncheck all the boxes.\n","\n","#@markdown ![Models](https://i.imgur.com/PEPqhpC.png)\n","\n","imagenet_1024 = False #@param {type:\"boolean\"}\n","imagenet_16384 = False #@param {type:\"boolean\"}\n","gumbel_8192 = True #@param {type:\"boolean\"}\n","coco = False #@param {type:\"boolean\"}\n","faceshq = False #@param {type:\"boolean\"}\n","wikiart_1024 = False #@param {type:\"boolean\"}\n","wikiart_16384 = False #@param {type:\"boolean\"}\n","sflckr = False #@param {type:\"boolean\"}\n","\n","#Below all added with last update. Results weren't very good.\n","ade20k = False #@param {type:\"boolean\"}\n","ffhq = False #@param {type:\"boolean\"}\n","celebahq = False #@param {type:\"boolean\"}\n","\n","if imagenet_1024:\n","  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 1024\n","  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'  #ImageNet 1024\n","if imagenet_16384:\n","  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n","#  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\n","  !gdown https://drive.google.com/uc?id=135CJAtmKvhyoqJEYK8EIdveNspj4qjdc #imagenet_16384\n","\n","#You can find more models at https://github.com/CompVis/taming-transformers\n","#It might cause a bug if this is left checked while the parameter is set to another model.\n","if gumbel_8192:\n","  !curl -L -o gumbel_8192.yaml -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #Gumbel 8192\n","  !curl -L -o gumbel_8192.ckpt -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #Gumbel 8192\n","\n","if coco:\n","  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml' #COCO\n","  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt' #COCO\n","if faceshq:\n","  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n","  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\n","if wikiart_1024: \n","  !curl -L -o wikiart_1024.yaml -C - 'http://dl.nmkd.de/ai/clip/wikiart-vqgan/WikiArt_augmented_Steps_7mil_finetuned_480k.yaml' #WikiArt 1024\n","#This host is slow and unstable, so I have it downloading from my Google Drive instead.\n","#  !curl -L -o wikiart_1024.ckpt -C - 'http://dl.nmkd.de/ai/clip/wikiart-vqgan/WikiArt_augmented_Steps_7mil_finetuned_480k.ckpt' #WikiArt 1024\n","  !gdown https://drive.google.com/uc?id=1X_iw1XP02B8q_Fz2s5qR5L-auyuoXQnt\n","\n","if wikiart_16384:\n","  !curl -L -o wikiart_16384.yaml -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml' #WikiArt 16384\n","#  !curl -L -o wikiart_16384.ckpt -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt' #WikiArt 16384\n","  !gdown https://drive.google.com/uc?id=1-B7tbvJpuEQJ0zjpoKVjOmNagbEdlORg #WikiArt 16384\n","if sflckr:\n","  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n","  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR\n","\n","#Below all added with last update. The results weren't very good.\n","if ade20k:\n","  !curl -L -o ade20k.yaml -C - 'https://static.miraheze.org/intercriaturaswiki/b/bf/Ade20k.txt' #ADE20K\n","#  !curl -L -o ade20k.ckpt -C - 'https://app.koofr.net/content/links/0f65c2cd-7102-4550-a2bd-07fd383aac9e/files/get/last.ckpt?path=%2F2020-11-20T21-45-44_ade20k_transformer%2Fcheckpoints%2Flast.ckpt' #ADE20K\n","  !gdown https://drive.google.com/uc?id=17njzwDGWOjlFwd0HZBzhylTDbaxiFc1C #ADE20K\n","if ffhq:\n","  !curl -L -o ffhq.yaml -C - 'https://app.koofr.net/content/links/0fc005bf-3dca-4079-9d40-cdf38d42cd7a/files/get/2021-04-23T18-19-01-project.yaml?path=%2F2021-04-23T18-19-01_ffhq_transformer%2Fconfigs%2F2021-04-23T18-19-01-project.yaml&force' #FFHQ\n","#  !curl -L -o ffhq.ckpt -C - 'https://app.koofr.net/content/links/0fc005bf-3dca-4079-9d40-cdf38d42cd7a/files/get/last.ckpt?path=%2F2021-04-23T18-19-01_ffhq_transformer%2Fcheckpoints%2Flast.ckpt&force' #FFHQ\n","  !gdown https://drive.google.com/uc?id=1b5uJRxwvUiXugjHKwoQrSCuI2gXf4-Fy #FFHQ\n","if celebahq:\n","  !curl -L -o celebahq.yaml -C - 'https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/2021-04-23T18-11-19-project.yaml?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fconfigs%2F2021-04-23T18-11-19-project.yaml&force' #CelebA-HQ\n","#  !curl -L -o celebahq.ckpt -C - 'https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/last.ckpt?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fcheckpoints%2Flast.ckpt&force' #CelebA-HQ\n","  !gdown https://drive.google.com/uc?id=1RB58pPji-VDuDmWuMNwIExU7V3u_0VDI #CelebA-HQ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sITI64XQ8JkR","cellView":"form"},"source":["#@markdown To use your drive, set `root_path` to the relative drive folder path you want outputs to be saved to, then execute the cell. Leaving the field blank or just not running this will have outputs save to the runtime temp storage.\n","root_path = \"\" #@param {type: \"string\"}\n","abs_root_path = \"/content\"\n","if len(root_path) > 0:\n","    abs_root_path = abs_root_path + \"/drive/MyDrive/\" + root_path\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tthw0YaispD"},"source":["## Tools for execution:\n","Mainly what you will have to modify will be `texts:`, there you can place the textprompt(s) you want to generate (separated with `|`). It is a list because you can put more than one text, and so the AI tries to 'mix' the images, giving the same priority to both texts.\n","\n","To use an initial image to the model, you just have to upload a file to the Colab environment (in the section on the left), and then modify `init_image:` putting the exact name of the file. Example: `sample.png`\n","\n","You can also modify the model by changing the line that says `model:`. Currently 1024, 16384, WikiArt, S-FLCKR and COCO-Stuff are available. To activate them you have to have downloaded them first, and then you can simply select it.\n","\n","You can also use `target_images`, which is basically putting one or more images on it that the AI will take as a \"target\", fulfilling the same function as putting text on it. To put more than one you have to use `|` as a separator."]},{"cell_type":"code","metadata":{"id":"ZdlpRFL8UAlW","cellView":"form"},"source":["prompts = \"a photorealistic image of a stained glass window | trending on artstation | unreal engine | logo:0 | U:0 | text:0\" #@param {type:\"string\"}\n","  # Use percent or parts for weight, e.g., \"unicole unicron:2|unreal engine:1|raytracing:1\"\n","  # Use a weight of zero for elements to NOT include in image, e.g., \"red:0\"\n","width =  614#@param {type:\"number\"}\n","height =  432#@param {type:\"number\"}\n","model_id = \"gumbel_8192\" #@param [\"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"gumbel_8192\", \"coco\", \"faceshq\", \"wikiart_1024\", \"wikiart_16384\", \"sflckr\", \"ade20k\", \"ffhq\", \"celebahq\"]\n","image_interval =  50#@param {type:\"number\"}\n","init_image = \"/content/intersubjectivity.PNG\"#@param {type:\"string\"}\n","target_images = None#@param {type:\"string\"}\n","#@markdown\n","seed_single = \"-1\"#@param {type:\"string\"}\n","seed_list = \"1,2,3\" #@param {type:\"string\"}\n","seed_type = seed_single #@param [\"seed_single\", \"seed_list\"] {type:\"raw\"}\n","#@markdown\n","max_iterations = 501#@param {type:\"number\"}\n","run_batch = True#@param {type:\"boolean\"}\n","batch_seed_count = 10#@param {type:\"number\"}\n","input_images = \"\"\n","\n","#@markdown Optional \"advanced\" options:\n","step_size = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n","\t  # A lower step size (default 0.1) or \"learning rate\" should be more stable.\n","cutn = 64#@param {type:\"number\"}\n","  \t# Number of \"cuts\" (default 64) higher is more detailed\n","cut_pow = 1.0#@param {type:\"number\"}\n","\t\t# Not sure what this does (default 1.0)\n","#cut_size = 1#@param {type:\"number\"}\n","\t\t# No idea what this does (default 1)\n","init_weight = 0.1#@param {type:\"number\"}\n","\t\t# No idea what this does (default 0.1)\n","#This will crop and resize between intervals for a \"zoom\" animation. It doesn't work yet.\n","zoom = False#@param {type:\"boolean\"}\n","\n","model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n","                 \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \n","                 \"gumbel_8192\":\"Gumbel 8192\",\"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\",\n","              \t \"ade20k\":\"ADE20K\",\"celebahq\":\"CelebA-HQ\",\"ffhq\":\"Flickr-Faces-HQ\"}\n","model_name = model_names[model_id]\n","\n","if not 'abs_root_path' in globals():\n","    abs_root_path = \"/content\"\n","\n","if init_image == \"None\":\n","    init_image = None\n","if target_images == \"None\" or not target_images:\n","    target_images = []\n","else:\n","    target_images = target_images.split(\"|\")\n","    target_images = [image.strip() for image in target_images]\n","\n","if init_image or target_images != []:\n","    input_images = True\n","\n","prompts_str = prompts\n","prompt_hash = hashlib.md5(prompts.encode())\n","prompt_md5 = prompt_hash.hexdigest()\n","\n","prompts = [frase.strip() for frase in prompts.split(\"|\")]\n","if prompts == ['']:\n","    prompts = []\n","\n","if seed_type == seed_single:\n","    seed = None if seed_single == -1 else seed_single  \n","else:\n","    seed_list = ''.join(seed_list)\n","    seed = seed_list.replace(\" \", \"\").split(',')\n","\n","args = argparse.Namespace(\n","    prompts=prompts,\n","    image_prompts=target_images,\n","    noise_prompt_seeds=[],\n","    noise_prompt_weights=[],\n","    size=[width, height],\n","    init_image=init_image,\n","    init_weight=0.,\n","    clip_model='ViT-B/32',\n","    vqgan_config=f'{model_id}.yaml',\n","    vqgan_checkpoint=f'{model_id}.ckpt',\n","    step_size=step_size,\n","    cutn=64,\n","    cut_pow=1.,\n","    display_freq=image_interval,\n","    seed=seed,\n","    batch_seed_count=(batch_seed_count if run_batch else 1),\n","    hash = prompt_md5,\n","    prompt_str = prompts_str,\n",")\n","\n","print(\"Prompt Hash: \", args.hash)\n","\n","!mkdir {abs_root_path}/{args.hash}\n","!rm -f {abs_root_path}/{args.hash}/prompt.txt\n","!echo \"{prompts_str}\" > {abs_root_path}/{args.hash}/prompt.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"wCjw65wogLSY"},"source":["#@title Load the model\n","\n","#@markdown This cell must be run after parameters are configured, but does NOT need to be run again when parameters are changed\n","\n","#@markdown Once this has been run successfully you only need to run parameters and then the program to execute with new parameters\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n","perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n","\n","cut_size = perceptor.visual.input_resolution\n","\n","if gumbel_8192:\n","    e_dim = model.quantize.embedding_dim\n","else:\n","    e_dim = model.quantize.e_dim\n","\n","f = 2**(model.decoder.num_resolutions - 1)\n","make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n","\n","toksX, toksY = args.size[0] // f, args.size[1] // f\n","sideX, sideY = toksX * f, toksY * f\n","\n","if gumbel_8192:\n","    e_dim = 256\n","    n_toks = model.quantize.n_embed\n","    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n","else:\n","  \te_dim = model.quantize.e_dim\n","  \tn_toks = model.quantize.n_e\n","  \tz_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","  \tz_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","\n","normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                std=[0.26862954, 0.26130258, 0.27577711])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wen7ZIJiFPJh","cellView":"form"},"source":["#@title Do the execution\n","\n","def synth(z):\n","    if gumbel_8192:\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n","    else:\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n","\n","    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n","\n","def add_xmp_data(nombrefichero):\n","    imagen = ImgTag(filename=nombrefichero)\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    if args.prompts:\n","        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    else:\n","        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', model_name, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'input_images',str(input_images) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    #for frases in args.prompts:\n","    #    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'Prompt' ,frases, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.close()\n","\n","def add_stegano_data(filename):\n","    data = {\n","        \"title\": \" | \".join(args.prompts) if args.prompts else None,\n","        \"notebook\": \"VQGAN+CLIP\",\n","        \"i\": i,\n","        \"model\": model_name,\n","        \"seed\": str(seed),\n","        \"input_images\": input_images\n","    }\n","    lsb.hide(filename, json.dumps(data)).save(filename)\n","\n","@torch.no_grad()\n","def checkin(i, losses):\n","    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n","    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n","    out = synth(z)\n","    TF.to_pil_image(out[0].cpu()).save('progress.png')\n","    #no one is looking at this info just save some cycles\n","    #add_stegano_data('progress.png')\n","    #add_xmp_data('progress.png')\n","    display.display(display.Image('progress.png'))\n","\n","def ascend_txt():\n","    global i\n","    global z\n","\n","    out = synth(z)\n","    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n","\n","    result = []\n","\n","    if args.init_weight:\n","        result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n","\n","    for prompt in pMs:\n","        result.append(prompt(iii))\n","    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n","    img = np.transpose(img, (1, 2, 0))\n","    filename = f\"steps/{i:04}.png\"\n","    imageio.imwrite(filename, np.array(img))\n","    add_stegano_data(filename)\n","    add_xmp_data(filename)\n","    return result\n","\n","def train(i):\n","    global opt\n","    global z\n","\n","    opt.zero_grad()\n","    lossAll = ascend_txt()\n","    if i % args.display_freq == 0:\n","        checkin(i, lossAll)\n","    loss = sum(lossAll)\n","    loss.backward()\n","    opt.step()\n","    with torch.no_grad():\n","        z.copy_(z.maximum(z_min).minimum(z_max))\n","\n","def make_final_name(s):\n","    stepstr = \"_s{s}\".format(s=str(int(args.step_size*100)).zfill(3))\n","    noiterstr = \"{path}/{hash}/{seed}{step}_\".format(path=abs_root_path, hash=args.hash, seed=str(s), step=stepstr)\n","    expr = noiterstr + \"(?P<iter>\\\\d{4})[.]png\"\n","    print(expr)\n","    newiter = max_iterations\n","    if(args.init_image is not None):\n","        if(re.fullmatch(expr, args.init_image) is not None):\n","            m = re.match(expr, args.init_image)\n","            iter = m.group(\"iter\")\n","            inum = int(iter)\n","            newiter = inum+newiter\n","    return noiterstr+str(str(newiter).zfill(4))+\".png\"\n","\n","def save_final(s):\n","    out = synth(z)\n","    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n","    img = np.transpose(img, (1, 2, 0))\n","    filename = make_final_name(s)\n","    imageio.imwrite(filename, np.array(img))\n","    add_stegano_data(filename)\n","    add_xmp_data(filename)\n","\n","def run_seed():\n","    torch.cuda.empty_cache()\n","    global opt\n","    global z\n","    global i\n","\n","    if args.init_image:\n","        pil_image = Image.open(args.init_image).convert('RGB')\n","        pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n","        z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n","    else:\n","        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n","\n","        if gumbel_8192:\n","          z = one_hot @ model.quantize.embed.weight\n","        else:\n","          z = one_hot @ model.quantize.embedding.weight\n","\n","        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n","    z_orig = z.clone()\n","    z.requires_grad_(True)\n","    opt = optim.Adam([z], lr=args.step_size)\n","    i = 0\n","    with tqdm() as pbar:\n","        while True:\n","            train(i)\n","            if i == max_iterations:\n","                break\n","            i += 1\n","            pbar.update()\n","\n","print('Using device:', device)\n","if prompts:\n","    print('Using texts:', prompts)\n","\n","if target_images:\n","    print('Using image prompts:', target_images)\n","\n","if args.batch_seed_count is None:\n","    batch_seed_count = 10\n","else:\n","    batch_seed_count = args.batch_seed_count\n","\n","pMs = []\n","\n","for prompt in args.prompts:\n","    txt, weight, stop = parse_prompt(prompt)\n","    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","for prompt in args.image_prompts:\n","    path, weight, stop = parse_prompt(prompt)\n","    img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n","    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n","    embed = perceptor.encode_image(normalize(batch)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n","    gen = torch.Generator().manual_seed(seed)\n","    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n","    pMs.append(Prompt(embed, weight).to(device))\n","\n","try:\n","    if type(seed) is list:\n","        for s in seed:\n","            clear_output(wait=True)\n","            torch.manual_seed(s)\n","            print('Using seed:', s)\n","            run_seed()\n","            save_final(s)\n","    else:\n","        while batch_seed_count > 0: #unchecking run_batch just forces the count to be 1\n","            clear_output(wait=True)\n","            if run_batch or (args.seed is None) or (int(args.seed) < 0):\n","                s = torch.seed()\n","            else:\n","                s = args.seed\n","            torch.manual_seed(s)\n","            print('Using seed:', s)\n","            run_seed()\n","            save_final(s)\n","            batch_seed_count -= 1\n","\n","except KeyboardInterrupt:\n","    del z\n","    del z_max\n","    del z_min\n","    del opt\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rY8xiZ7ybT4g","cellView":"form"},"source":["#@title Compress batch results\n","\n","!mkdir /content/old_results\n","!zip -r /content/batch_results.zip {abs_root_path}/{args.hash}\n","!mv {abs_root_path}/{args.hash} /content/old_results/{args.hash}\n","\n","!cp batch_results.zip /content/drive/MyDrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFo5vz0UYBrF","cellView":"form"},"source":["#@title Generate a video with the results\n","#@markdown Start and end points are percentages, default 5 and 100 just cuts off the gray noise at the beginning. Target length is in seconds.\n","#If you did a batch, this will generate a video only for the last image.\n","\n","reset_value = i\n","\n","start_point = 5 #@param {type:\"slider\", min:1, max:100, step:1}\n","end_point = 100 #@param {type:\"slider\", min:1, max:100, step:1}\n","\n","init_frame = int(i*start_point/100)\n","last_frame = int(i*end_point/100)\n","\n","min_fps = 10\n","max_fps = 30\n","\n","total_frames = last_frame-init_frame\n","\n","target_length = 15 #@param {type:\"number\"} #Video length in seconds.\n","length = target_length\n","\n","frames = []\n","tqdm.write('Generating video...')\n","for i in range(init_frame,last_frame): #\n","    filename = f\"steps/{i:04}.png\"\n","    frames.append(Image.open(filename))\n","\n","fps = np.clip(total_frames/length,min_fps,max_fps)\n","\n","from subprocess import Popen, PIPE\n","p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n","for im in tqdm(frames):\n","    im.save(p.stdin, 'PNG')\n","p.stdin.close()\n","\n","print(\"The video is now being compressed, wait ... \")\n","p.wait()\n","print(\"The video is ready.\")\n","\n","i = reset_value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8lvN6b0mb-b","cellView":"form"},"source":["# @title View video in browser\n","# @markdown This process may take a little longer. If you don't want to wait, download it by executing the next cell instead of using this cell.\n","\n","mp4 = open('video.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","#Uncomment below line to show your prompt above the video.\n","print(prompts)\n","display.HTML(\"\"\"\n","<video width=400 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y0e8pHyJmi7s"},"source":["#@title Download video\n","#This is kind of dumb, since you can just right-click to download.\n","#Use this cell for something else.\n","\n","from google.colab import files\n","files.download(\"video.mp4\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6Ar_Knc9TSS","cellView":"form"},"source":["#@title Optional CKPT file transfer\n","#@markdown Use this to make large files downloadable from Google Drive.\n","\n","# Uncomment only the lines for the step you want to do. Do 1, then 2 to save the file(s). \n","# Next time do 1, then 3 to copy the file(s) over instead of downloading.\n","# Change the filename to match the name of the model you want to save or load.\n","\n","# 1. Mount your Google Drive.\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","# 2. Copy the CKPT file TO your Drive. (Do this for each model you want to save.)\n","#!cp wikiart_16384.ckpt /content/drive/MyDrive\n","\n","# 3. Copy the CKPT file FROM your Drive.\n","#!cp /content/drive/MyDrive/ade20k.ckpt ade20k.ckpt\n","#!cp /content/drive/MyDrive/ffhq.ckpt ffhq.ckpt\n","#!cp /content/drive/MyDrive/celebahq.ckpt celebahq.ckpt\n","#!cp /content/drive/MyDrive/wikiart_1024.ckpt wikiart_1024.ckpt\n","\n","# 4. Download only the YMAL file you need. (They're very small.)\n","# Just copy the line from the model selection step.\n","#!curl -L -o ade20k.yaml -C - 'https://static.miraheze.org/intercriaturaswiki/b/bf/Ade20k.txt' #ADE20K\n","#!curl -L -o ffhq.yaml -C - 'https://app.koofr.net/content/links/0fc005bf-3dca-4079-9d40-cdf38d42cd7a/files/get/2021-04-23T18-19-01-project.yaml?path=%2F2021-04-23T18-19-01_ffhq_transformer%2Fconfigs%2F2021-04-23T18-19-01-project.yaml&force' #FFHQ\n","#!curl -L -o celebahq.yaml -C - 'https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/2021-04-23T18-11-19-project.yaml?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fconfigs%2F2021-04-23T18-11-19-project.yaml&force' #CelebA-HQ\n","#!curl -L -o wikiart_1024.yaml -C - 'http://dl.nmkd.de/ai/clip/wikiart-vqgan/WikiArt_augmented_Steps_7mil_finetuned_480k.yaml' #WikiArt 1024\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fb8jj9d9zHL7"},"source":["# To prevent automatic disconnect due to inactivity:\n","\n","Open inspector (ctrl+shift+i), click on console, and run this code in it (ctrl+enter):\n","\n","```javascript\n","function ClickConnect() {\n","  console.log('Working')\n","  document\n","    .querySelector('#top-toolbar > colab-connect-button')\n","    .shadowRoot.querySelector('#connect')\n","    .click()\n","}\n","intervalTiming = setInterval(ClickConnect, 60000)\n","```\n","\n","It could crash the browser!\n","\n","If that doesn't work, try this:\n","\n","```javascript\n","function ClickConnect(){\n","  console.log('Working');\n","  document.querySelector('colab-toolbar-button#toolbar-add-text').click();\n","}\n","intervalTiming = setInterval(ClickConnect, 60000);\n","```"]}]}